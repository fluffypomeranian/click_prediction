where t.ad_id = p.ad_id and p.document_id = d.document_id
limit 500000;"
merged_table=dbGetQuery(connection, join_query)
head(merged_table, 3)
dim(merged_table)
# CREATE AND WRITE NEW TABLE
dbWriteTable(connection, "merged_table", merged_table, row.names=FALSE)
# look at the new table
getMergedTable="select * from merged_table limit 500000"
new_table = dbGetQuery(connection, getMergedTable)
# list the structure of mydata
str(new_table)
# CREATE TRAINING AND TEST SET
partition_size = floor(0.80 * nrow(new_table)) ## 80% of the sample size
set.seed(123) ## set the seed to make your partition reproductible
partition_index <- sample(seq_len(nrow(new_table)), size = partition_size)
local_train_set <- new_table[partition_index, ]
local_test_set <- new_table[-partition_index, ]
# LOCAL TEST SET
dim(local_test_set)
head(local_test_set, n=5)
# LOCAL TRAIN SET
dim(local_train_set)
head(local_train_set, n=5)
ones = new_table[new_table$clicked>0, 50000]
length(ones)
length(new_table)
table(new_table$clicked)
onesVersion2 = ones = subset(new_table, clicked>0)
dim(onesVersion2)
zeros = sample(new_table$clicked=0, 50000)
zeros = sample(new_table$clicked<1, 50000)
dim(zeros)
zeros = subset(new_table, clicked = 0)
dim(zeros)
undersampled_zeros =apply(zeros, 1, sample, 50000)
zeros = subset(new_table, clicked > 1)
zeros = subset(new_table, clicked < 1)
#zeros = sample(new_table$clicked<1, 50000)
dim(zeros)
undersampled_zeros =apply(zeros, 1, sample, 50000)
undersampled_zeros =apply(zeros, 1, sample, 450014)
undersampled_zeros =apply(zeros, 1, sample, 450013)
undersampled_zeros =apply(zeros, 50000, sample, 1)
undersampled_zeros =apply(zeros, 50000, sample, 1)
table(zeros)
table(zeros$clicked)
table(ones$clicked)
undersampled_zeros = new_table[sample(nrow(new_table), 50000), ]
table(zeros$clicked)
table(ones$clicked)
table(undersampled_zeros$clicked)
undersampled_zeros = zeros[sample(nrow(zeros), 50000), ]
table(undersampled_zeros$clicked)
table(ones$clicked)
dim(undersampled_zeros)
dim(ones)
names(undersampled_zeros)
names(ones)
final_dataset<- data.frame(ones, undersampled_zeros )
final_dataset <- merge(undersampled_zeros, ones )
dim(final_dataset)
final_dataset <- merge(undersampled_zeros, ones, all.x=TRUE)
dim(final_dataset)
final_dataset <- merge(undersampled_zeros, ones, all.x=TRUE, all.y=TRUE)
dim(final_dataset)
# CREATE TRAINING AND TEST SET
partition_size = floor(0.80 * nrow(final_dataset)) ## 80% of the sample size
set.seed(123) ## set the seed to make your partition reproductible
partition_index <- sample(seq_len(nrow(final_dataset)), size = partition_size)
local_train_set <- final_dataset[partition_index, ]
local_test_set <- final_dataset[-partition_index, ]
# LOCAL TEST SET
dim(local_test_set)
head(local_test_set, n=5)
# LOCAL TRAIN SET
dim(local_train_set)
head(local_train_set, n=5)
rf.final_data_set=randomForest(clicked~. -clicked,data=final_dataset,subset=local_train_set)
rf.boston
rf.final_data_set=randomForest(clicked~. -clicked,data=final_dataset,subset=local_train_set)
rf.final_data_set
#install.packages('randomForest', repos="http://cran.r-project.org")
require(randomForest)
set.seed(101)
dim(final_dataset)
rf.final_data_set=randomForest(clicked~. -clicked,data=final_dataset,subset=local_train_set)
rf.final_data_set
attach(local_train_set)
rf.final_data_set=randomForest(clicked~. -clicked,data=final_dataset,subset=local_train_set)
attach(local_train_set)
rf.final_data_set=randomForest(clicked~. -clicked,data=final_dataset,subset=local_train_set)
rf.final_data_set
rf.final_data_set=randomForest(local_train_set$clicked~. -local_train_set$clicked,data=final_dataset,subset=local_train_set)
rf.final_data_set=randomForest(local_train_set~. -local_train_set$clicked,data=final_dataset,subset=local_train_set)
rf.final_data_set=randomForest(local_train_set. -local_train_set$clicked,data=final_dataset,subset=local_train_set)
str(local_train_set)
summary(local_train_set)
attach(final_dataset)
fit=randomForest(clicked~., data=final_dataset,subset=local_train_set)
traceback()
varNames <- names(local_train_set)
# Exclude ID or Response variable
varNames <- varNames[!varNames %in% c("clicked")]
# add + sign between exploratory variables
varNames1 <- paste(varNames, collapse = "+")
summary(varNames1)
varNames1
rf.form <- as.formula(paste("y", varNames1, sep = " ~ "))
rf.form
tree_arg <- as.formula(paste("y", varNames1, sep = " ~ "))
fit=randomForest(tree_arg, data=final_dataset,subset=local_train_set)
tree_arg <- as.formula(paste("clicked", varNames1, sep = " ~ "))
fit=randomForest(tree_arg, data=final_dataset,subset=local_train_set)
fit=randomForest(tree_arg, data=final_dataset,subset=local_train_set, ntree=500,importance=T)
dbExistsTable(connection, "clicks_train")
########################### Readme ###############################
#
#   Author:       Tim Siwula
#   Proposal:     http://bit.ly/2gcCLQ4
#   Kaggle:       http://bit.ly/2gMVpPG
#   Github:       http://bit.ly/2gZoTwy
#   Data:         http://bit.ly/2fQ0LHW
#
##################################################################
########################### Notes ################################
# clear workspace ----> rm(list = ls())
#
####################################################################################
########################### SET UP DATABASE CONNECTION ################################
# SET UP DATABASE CONNECTION
require("RPostgreSQL")    #install.packages("RPostgreSQL")
driver <- dbDriver("PostgreSQL")   # loads the PostgreSQL driver
# creates a connection to the postgres database
# note that "con" will be used later in each connection to the database
connection <- dbConnect(driver, dbname = "clickprediction",
host = "localhost", port = 5432,
user = "admin", password = "admin")
# confirm the tables are accessible
dbExistsTable(connection, "clicks_train")
####################################################################################
########################### QUERY THE DATABASE ########################################
# QUERY THE DATABASE
# 1)
# try to find features related to ad_id.
# here we join click_train and promoted-content with ad_id.
# look at clicks_train first
getClicksTrain="select * from clicks_train limit 10 "
clicks_train = dbGetQuery(connection, getClicksTrain)
clicks_train
# look at promoted_content next
getPromotedContent="select * from promoted_content limit 10"
promoted_content = dbGetQuery(connection, getPromotedContent)
promoted_content
# join click_train and promoted-content with ad_id new table
# 500k apears to be stable with rstudio.
join_query = "
select t.display_id, t.ad_id, t.clicked, d.document_id,
d.topic_id, d.confidence_level
from clicks_train t, promoted_content p, documents_topics d
where t.ad_id = p.ad_id and p.document_id = d.document_id
limit 500000;"
merged_table=dbGetQuery(connection, join_query)
head(merged_table, 3)
dim(merged_table)
####################################################################################
########################### QUERY THE DATABASE ########################################
# QUERY THE DATABASE
# 1)
# try to find features related to ad_id.
# here we join click_train and promoted-content with ad_id.
# look at clicks_train first
getClicksTrain="select * from clicks_train limit 10 "
clicks_train = dbGetQuery(connection, getClicksTrain)
clicks_train
# look at promoted_content next
getPromotedContent="select * from promoted_content limit 10"
promoted_content = dbGetQuery(connection, getPromotedContent)
promoted_content
# join click_train and promoted-content with ad_id new table
# 500k apears to be stable with rstudio.
join_query = "
select t.display_id, t.ad_id, t.clicked, d.document_id,
d.topic_id, d.confidence_level
from clicks_train t, promoted_content p, documents_topics d
where t.ad_id = p.ad_id and p.document_id = d.document_id
limit 500000;"
merged_table=dbGetQuery(connection, join_query)
head(merged_table, 3)
dim(merged_table)
####################################################################################
########################### CREATE AND WRITE NEW TABLE #############################
# CREATE AND WRITE NEW TABLE
dbWriteTable(connection, "merged_table", merged_table, row.names=FALSE)
# look at the new table
getMergedTable="select * from merged_table limit 500000"
new_table = dbGetQuery(connection, getMergedTable)
# list the structure of mydata
str(new_table)
####################################################################################
########################### GET 50/50 CLICKED ###########################
ones = new_table[new_table$clicked>0, 50000]
length(ones)
length(new_table)
onesVersion2 = ones = subset(new_table, clicked>0)
dim(onesVersion2)
zeros = subset(new_table, clicked < 1)
dim(zeros)
undersampled_zeros = zeros[sample(nrow(zeros), 50000), ]
table(undersampled_zeros$clicked)
table(ones$clicked)
dim(undersampled_zeros)
dim(ones)
names(undersampled_zeros)
names(ones)
final_dataset<- data.frame(ones, undersampled_zeros )
final_dataset <- merge(undersampled_zeros, ones, all.x=TRUE, all.y=TRUE)
# FINAL DATA SET
dim(final_dataset)
head(final_dataset, n=5)
#########################################################################
########################### CREATE TRAINING AND TEST SET ###########################
# CREATE TRAINING AND TEST SET
partition_size = floor(0.80 * nrow(final_dataset)) ## 80% of the sample size
set.seed(123) ## set the seed to make your partition reproductible
partition_index <- sample(seq_len(nrow(final_dataset)), size = partition_size)
local_train_set <- final_dataset[partition_index, ]
local_test_set <- final_dataset[-partition_index, ]
# LOCAL TEST SET
dim(local_test_set)
head(local_test_set, n=5)
# LOCAL TRAIN SET
dim(local_train_set)
head(local_train_set, n=5)
#########################################################################
########################### RANDOM FORESTS ###########################
##########################   INIT       ##############################
#install.packages('randomForest', repos="http://cran.r-project.org")
require(randomForest)
set.seed(101)
dim(final_dataset)
attach(final_dataset)
#########################################################################
##########################   FIT W/RESPONSE CLICKED        ##############
#attach(local_train_set)
summary(local_train_set)
# CREATE REGRESSION ARGUMENT WITH ALL COLUMNS EXCLUDING CLICKED
varNames <- names(local_train_set)
# Exclude ID or Response variable
varNames <- varNames[!varNames %in% c("clicked")]
# add + sign between exploratory variables
varNames1 <- paste(varNames, collapse = "+")
# Add response variable and convert to a formula object
tree_arg <- as.formula(paste("clicked", varNames1, sep = " ~ "))
fit=randomForest(tree_arg, data=final_dataset,subset=local_train_set, ntree=500,importance=T)
rf.final_data_set
str(local_train_set)
#########################################################################
#########################################################################
fit=randomForest(tree_arg, data=final_dataset,subset=local_train_set, ntree=500,importance=T)
fit=randomForest(tree_arg, data=final_dataset,subset=local_train_set)
final_dataset <- merge(undersampled_zeros, ones, all.x=FALSE, all.y=FALSE)
# FINAL DATA SET
dim(final_dataset)
final_dataset <- merge(undersampled_zeros, ones, all.x=TRUE, all.y=FALSE)
# FINAL DATA SET
dim(final_dataset)
head(final_dataset, n=5)
final_dataset <- merge(undersampled_zeros, ones, all.x=TRUE, all.y=TRUE)
# FINAL DATA SET
dim(final_dataset)
head(final_dataset, n=5)
install.packages('randomForest', repos="http://cran.r-project.org")
fit=randomForest(tree_arg., data=final_dataset,subset=local_train_set)
varNames <- names(local_train_set)
# Exclude ID or Response variable
varNames <- varNames[!varNames %in% c("clicked")]
# add + sign between exploratory variables
varNames1 <- paste(varNames, collapse = "+")
# Add response variable and convert to a formula object
tree_arg <- as.formula(paste("clicked", varNames1, sep = " ~ "))
fit=randomForest(tree_arg., data=final_dataset,subset=local_train_set)
tree_arg
fit=randomForest(tree_arg, data=final_dataset,subset=local_train_set)
?as.data.frame()
str(final_dataset)
str(local_train_set)
str(tree_arg)
fit=randomForest(clicked ~ display_id + ad_id + document_id + topic_id + confidence_level, data=final_dataset,subset=local_train_set)
rf.final_data_set
rf.fit=randomForest(clicked ~ display_id + ad_id + document_id + topic_id + confidence_level, data=final_dataset,subset=local_train_set)
tree_arg=as.data.frame(tree_arg)
attach(local_train_set)
attach(final_dataset)
clear workspace ----> rm(list = ls())
rm(list = ls())
dbExistsTable(connection, "clicks_train")
########################### SET UP DATABASE CONNECTION ################################
# SET UP DATABASE CONNECTION
require("RPostgreSQL")    #install.packages("RPostgreSQL")
driver <- dbDriver("PostgreSQL")   # loads the PostgreSQL driver
# creates a connection to the postgres database
# note that "con" will be used later in each connection to the database
connection <- dbConnect(driver, dbname = "clickprediction",
host = "localhost", port = 5432,
user = "admin", password = "admin")
# confirm the tables are accessible
dbExistsTable(connection, "clicks_train")
####################################################################################
########################### QUERY THE DATABASE ########################################
# QUERY THE DATABASE
# 1)
# try to find features related to ad_id.
# here we join click_train and promoted-content with ad_id.
# look at clicks_train first
getClicksTrain="select * from clicks_train limit 10 "
clicks_train = dbGetQuery(connection, getClicksTrain)
clicks_train
# look at promoted_content next
getPromotedContent="select * from promoted_content limit 10"
promoted_content = dbGetQuery(connection, getPromotedContent)
promoted_content
# join click_train and promoted-content with ad_id new table
# 500k apears to be stable with rstudio.
join_query = "
select t.display_id, t.ad_id, t.clicked, d.document_id,
d.topic_id, d.confidence_level
from clicks_train t, promoted_content p, documents_topics d
where t.ad_id = p.ad_id and p.document_id = d.document_id
limit 500000;"
merged_table=dbGetQuery(connection, join_query)
head(merged_table, 3)
dim(merged_table)
####################################################################################
########################### CREATE AND WRITE NEW TABLE #############################
# CREATE AND WRITE NEW TABLE
dbWriteTable(connection, "merged_table", merged_table, row.names=FALSE)
# look at the new table
getMergedTable="select * from merged_table limit 500000"
new_table = dbGetQuery(connection, getMergedTable)
# list the structure of mydata
str(new_table)
####################################################################################
########################### GET 50/50 CLICKED ###########################
ones = new_table[new_table$clicked>0, 50000]
length(ones)
length(new_table)
onesVersion2 = ones = subset(new_table, clicked>0)
dim(onesVersion2)
zeros = subset(new_table, clicked < 1)
dim(zeros)
undersampled_zeros = zeros[sample(nrow(zeros), 50000), ]
table(undersampled_zeros$clicked)
table(ones$clicked)
dim(undersampled_zeros)
dim(ones)
names(undersampled_zeros)
names(ones)
final_dataset <- merge(undersampled_zeros, ones, all.x=TRUE, all.y=TRUE)
# FINAL DATA SET
dim(final_dataset)
head(final_dataset, n=5)
#########################################################################
########################### CREATE TRAINING AND TEST SET ###########################
# CREATE TRAINING AND TEST SET
partition_size = floor(0.80 * nrow(final_dataset)) ## 80% of the sample size
set.seed(123) ## set the seed to make your partition reproductible
partition_index <- sample(seq_len(nrow(final_dataset)), size = partition_size)
local_train_set <- final_dataset[partition_index, ]
local_test_set <- final_dataset[-partition_index, ]
# LOCAL TEST SET
dim(local_test_set)
head(local_test_set, n=5)
# LOCAL TRAIN SET
dim(local_train_set)
head(local_train_set, n=5)
#########################################################################
clicked_model2=glm(clicked~ .,data=clicks_train,family=binomial)
summary(clicked_model2)
model_pred_prob=predict(clicked_model2,clicked_test,type="response")
clicked_test
partition_size = floor(0.80 * nrow(final_dataset)) ## 80% of the sample size
set.seed(123) ## set the seed to make your partition reproductible
partition_index <- sample(seq_len(nrow(final_dataset)), size = partition_size)
# set training set
local_train_set <- final_dataset[partition_index, ]
local_train_set = sample(local_train_set, length(local_train_set))
# set test set
local_test_set <- final_dataset[-partition_index, ]
local_test_set = sample(local_test_set, length(local_test_set))
# LOCAL TEST SET
dim(local_test_set)
head(local_test_set, n=5)
# LOCAL TRAIN SET
dim(local_train_set)
head(local_train_set, n=5)
#########################################################################
################   FIT REGRESSION TREE TO TRAINING SET     ##############
# fit on clicked all features excluding clicked using the final dataset
clicked_tree = tree(clicked~.-clicked,final_dataset)
library(knitr)
library(markdown)
library(ISLR)
library(tree)
require("RPostgreSQL")    #install.packages("RPostgreSQL")
require(randomForest)     #install.packages('randomForest', repos="http://cran.r-project.org")
require(tree)             #install.packages("tree")
require(knitr)
clicked_tree = tree(clicked~.-clicked,final_dataset)
summary(clicked_tree)
plot(clicked_tree)
text(clicked_tree,pretty=0)
# SEE IF PRUING THE TREE WILL IMPROVE PERFORMANCE
cv_clicked_tree = cv.tree(clicked_tree)
plot(cv_clicked_tree$size, cv_clicked_tree$dev, type='b', main="cross-validation default")
pruned = prune.tree(clicked_tree, best=5)
plot(pruned)
text(pruned,pretty=0)
# USE UNPRUNED TREE TO MAKE PREDICTIONS ON THE TEST SET
# IF BETTER ...
yhat = predict(clicked_tree, newdata = final_dataset[-partition_index, ])
clicked_test=final_dataset[-partition_index, "clicked"]
plot(yhat,clicked_test)
abline(0,1)
mean((yhat-clicked_test)^2)
head(yhat, n=5)
dim(yhat)
str(yhat)
length(yhat)
summary(yhat)
attach(yhat)
# the test set MSE associated with the regreeesion tree
# is 0.21
#########################################################################
################   LOGISTIC REGRESSION    ##############
summary(final_dataset)
cor(final_dataset)
clicked_model2=glm(clicked~ .,data=clicks_train,family=binomial)
summary(clicked_model2)
#accuracy of clicked_model2
model_pred_prob=predict(clicked_model2,clicked_test,type="response")
model_pred_prob=predict(clicked_model2, clicked_test, type = "response")
summary(clicked_model2)
model_pred_prob=predict(clicked_model2, clicked_test, type = "response")
predict()
?predict()
model_pred_prob=predict(clicked_model2, clicked_test, type = "response")
model_pred_prob=predict(clicked_model2, newdata=clicked_test, type = "response")
model_pred_prob=predict(clicked_model2, newdata=clicked_test, type = "response")
model_pred_prob=predict(clicked_model2, final_dataset[-partition_index,], type = "response")
head(local_train_set, n=5)
dim(local_test_set)
head(local_test_set, n=5)
n_occur <- data.frame(table(clicks_train$display_id))
n_occur <- data.frame(table(vocabulary$id))
n_occur
dim(n_occur)
n_occur <- data.frame(table(final_dataset$display_id))
dim(n_occur)
dbGetQuery(connection, "select count(*) from merged_table")
driver <- dbDriver("PostgreSQL")   # loads the PostgreSQL driver
# creates a connection to the postgres database
# note that "con" will be used later in each connection to the database
connection <- dbConnect(driver, dbname = "clickprediction",
host = "localhost", port = 5432,
user = "admin", password = "admin")
# confirm the tables are accessible
dbExistsTable(connection, "clicks_train")
##################################################################################
dbGetQuery(connection, "select count(*) from merged_table")
dbGetQuery(connection, "select count(*) from clicks_train")
dbGetQuery(connection, "select count(*) from clicks_train unique")
dbGetQuery(connection, "select count(*) from (select distinct display_id from clicks_train)")
dbGetQuery(connection, "select count(*) from (select distinct display_id from clicks_train);")
dbGetQuery(connection, "select count(*) from select distinct display_id from clicks_train")
dbGetQuery(connection, "select count(distinct(display_id))")
dbGetQuery(connection, "count(distinct display_id) from clicks_train")
dbGetQuery(connection, "count(distinct display_id) from clicks_train;")
n_occur[n_occur$Freq > 1,]
dim(n_occur[n_occur$Freq > 1,])
dim(!n_occur[n_occur$Freq > 1,])
dim(!n_occur[n_occur$Freq = 1,])
dim(!n_occur[n_occur$Freq < 2,])
dim(n_occur[n_occur$Freq < 2,])
model_pred_prob=predict(clicked_model2, final_dataset[-partition_index,], type = "response")
length(model_pred_prob)
pred_direction=rep("Down", length(model_pred_prob))
pred_direction[model_pred_prob > 0.5] = "Up"
table(pred_direction, final_dataset[-partition_index,])
dim(final_dataset[-partition_index,])
dim(pred_direction)
length(pred_direction)
table(pred_direction, final_dataset[-partition_index,])
clicked_model2=glm(clicked~ .,data=clicks_train,family=binomial)
summary(clicked_model2)
#accuracy of clicked_model2
model_pred_prob=predict(clicked_model2, final_dataset[-partition_index,], type = "response")
# predicted ups and downs
pred_direction=rep("Down", length(model_pred_prob))
pred_direction[model_pred_prob > 0.5] = "Up"
#confusion matrix --> check accuracy
table(pred_direction, final_dataset[-partition_index,])
attach(final_dataset)
training = (nrow(clicked)*.80)
testing = !training
training = (nrow(final_dataset$clicked)*.80)
testing = !training
dim(final_dataset)
dim(final_dataset$clicked)
length(final_dataset$clicked)
training = (length(final_dataset$clicked)*.80)
training = (floor(length(final_dataset$clicked)*.80))
